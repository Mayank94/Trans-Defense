{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import options\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# Hyperparameters\n",
    "RANDOM_SEED = 1\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 25\n",
    "\n",
    "# Architecture\n",
    "NUM_FEATURES = 32*32\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Other\n",
    "DEVICE = \"cuda:0\"\n",
    "# DEVICE = \"cpu\"\n",
    "GRAYSCALE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "\n",
    "##########################\n",
    "### MNIST DATASET\n",
    "##########################\n",
    "\n",
    "# Note transforms.ToTensor() scales input images\n",
    "# to 0-1 range\n",
    "# Configure data loader\n",
    "\n",
    "os.makedirs(path, exist_ok=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        path,\n",
    "        train= True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(32),transforms.ToTensor()]           # transforms.Normalize([0.5], [0.5])  - we don't have to do the transformations coz it is already normalized to [0,1]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        path,\n",
    "        train= False,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [ transforms.Resize(32),transforms.ToTensor()]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier\n",
    "\n",
    "# Classfier Model - RESNET34\n",
    "\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes, grayscale):\n",
    "        self.inplanes = 64\n",
    "        if grayscale:\n",
    "            in_dim = 1\n",
    "        else:\n",
    "            in_dim = 3\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_dim, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, (2. / n)**.5)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):       # this is the method that will be used by libraries accessing my model, especially for torchattacks\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.fc(x)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits\n",
    "\n",
    "    def my_forward(self, x,extract_map = False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        #print(\"pre_maxpool\",x.shape)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        #print(\"after conv1\", x.shape)\n",
    "        x = self.layer1(x)\n",
    "        #print(\"after layer1\", x.shape)\n",
    "        x = self.layer2(x)\n",
    "        #print(\"after layer2\", x.shape)\n",
    "        x = self.layer3(x)\n",
    "        #print(\"after layer3\", x.shape)\n",
    "        x = self.layer4(x)\n",
    "        #print(\"after layer4\", x.shape)\n",
    "        #x = self.avgpool(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        if extract_map:\n",
    "\n",
    "            return x\n",
    "\n",
    "        #print(x.shape)             # now the x contains my feature map flattened\n",
    "        logits = self.fc(x)\n",
    "        #print(logits.shape)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        #print(probas.shape)\n",
    "        return logits, probas\n",
    "\n",
    "def resnet34(num_classes):\n",
    "    \"\"\"Constructs a ResNet-34 model.\"\"\"\n",
    "    model = ResNet(block=BasicBlock,\n",
    "                   layers=[3, 4, 6, 3],\n",
    "                   num_classes=NUM_CLASSES,\n",
    "                   grayscale=GRAYSCALE)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training the classifier\n",
    "\n",
    "classifier = ResNet34().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr= LEARNING_RATE)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits, probas = model.my_forward(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier Training\n",
    "\n",
    "start_time = time.time()\n",
    "best_acc = 0\n",
    "for epoch in range(10):\n",
    "\n",
    "    classifier.train()\n",
    "    for batch_idx, (features, targets) in enumerate(dataloader):\n",
    "\n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        #print(features.shape)\n",
    "        ### FORWARD AND BACK PROP\n",
    "        outputs = classifier(features)\n",
    "        #print(logits.shape)\n",
    "        #print(probas.shape)\n",
    "        cost = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        cost.backward()\n",
    "\n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "\n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
    "                   %(epoch+1, NUM_EPOCHS, batch_idx,\n",
    "                     len(dataloader), cost))\n",
    "\n",
    "\n",
    "\n",
    "    classifier.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False): # save memory during inference\n",
    "        acc = compute_accuracy(classifier, test_loader, device=DEVICE)\n",
    "        print('Epoch: %03d/%03d | Train: %.3f%%' % (\n",
    "              epoch+1, NUM_EPOCHS, acc\n",
    "              ))\n",
    "    if acc>best_acc:\n",
    "        torch.save(classifier, filename1)           #filename is the path where you want to save your classifer\n",
    "        best_acc = acc\n",
    "\n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating adversrial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchattacks\n",
    "atk1 = torchattacks.FGSM(classifier, eps=0.08)\n",
    "atk2 = torchattacks.PGD(classifier, eps=0.05, alpha=2/255, steps=10)\n",
    "atk3 = torchattacks.BIM(classifier, eps=0.05, alpha=2/255, steps=10)\n",
    "\n",
    "atk = torchattacks.MultiAttack([atk1,atk2,atk3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_train = []\n",
    "\n",
    "for i, (imgs, labels) in enumerate(dataloader):\n",
    "\n",
    "        # # Configure input\n",
    "        imgs = imgs.to(DEVICE)          # clean imgs\n",
    "\n",
    "        labels = labels.to(DEVICE)       # clean labels\n",
    "\n",
    "\n",
    "        attacked_imgs = atk(imgs,labels)        # attacked imgs\n",
    "\n",
    "        adv_train.extend(list(zip(imgs,attacked_imgs,labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_test = []\n",
    "\n",
    "for i, (imgs, labels) in enumerate(test_loader):\n",
    "\n",
    "        # Configure input\n",
    "        imgs = imgs.to(DEVICE)\n",
    "\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "\n",
    "        attacked_imgs = atk(imgs,labels)\n",
    "\n",
    "        adv_test.extend(list(zip(imgs,attacked_imgs,labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256  # Adjust this as needed\n",
    "adv_train_loader = DataLoader(adv_train, batch_size=batch_size, shuffle=False)\n",
    "adv_test_loader = DataLoader(adv_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DWT Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dwt_batch(batch_img):\n",
    "    # Convert the batch of images to NumPy array\n",
    "    batch_np = batch_img.to(\"cpu\").numpy()\n",
    "\n",
    "    # Perform DWT on the batch of images\n",
    "    LL_batch, (LH_batch, HL_batch, HH_batch) = pywt.dwt2(batch_np, \"haar\")\n",
    "\n",
    "    # Convert the DWT coefficients back to PyTorch tensors\n",
    "    LL_batch = torch.tensor(LL_batch)\n",
    "    LH_batch = torch.tensor(LH_batch)\n",
    "    HL_batch = torch.tensor(HL_batch)\n",
    "    HH_batch = torch.tensor(HH_batch)\n",
    "\n",
    "    return LL_batch, LH_batch, HL_batch, HH_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting and visualizing the adversarial attack on frequency domain through plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(adv_train_loader))\n",
    "clean = b[0]\n",
    "noisy = b[1]\n",
    "targets = b[2]\n",
    "\n",
    "LL, LH, HL, HH = dwt_batch(clean)\n",
    "LL_1, LH_1, HL_1, HH_1 = dwt_batch(noisy)\n",
    "\n",
    "# DWT plot of clean\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(torch.permute(LL[0],(1,2,0)).to(\"cpu\"))\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(torch.log(1+torch.permute(LH[0],(1,2,0)).to(\"cpu\")))\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(torch.log(1+torch.permute(HL[0],(1,2,0)).to(\"cpu\")))\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(torch.log(1+torch.permute(HH[0],(1,2,0)).to(\"cpu\")))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# DWT plot of noisy\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(torch.permute(LL_1[0],(1,2,0)).to(\"cpu\"))\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(torch.log(1+torch.permute(LH_1[0],(1,2,0)).to(\"cpu\")))\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(torch.log(1+torch.permute(HL_1[0],(1,2,0)).to(\"cpu\")))\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(torch.log(1+torch.permute(HH_1[0],(1,2,0)).to(\"cpu\")))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoiser Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import restormer_arch as base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bicubic_downsample(image, scale_factor=2):\n",
    "\n",
    "    new_height = int(image.size(-2) / scale_factor)\n",
    "    new_width = int(image.size(-1) / scale_factor)\n",
    "\n",
    "    downsampled_image = F.interpolate(image, size=(new_height, new_width), mode='bicubic', align_corners=False)\n",
    "\n",
    "    return downsampled_image\n",
    "\n",
    "def bicubic_upsample(image, scale_factor=2):\n",
    "    \n",
    "    upsampled_image = F.interpolate(image, scale_factor=scale_factor, mode='bicubic', align_corners=False)\n",
    "\n",
    "    return upsampled_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, bias):\n",
    "        super(CrossAttention, self).__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "\n",
    "        self.q = nn.Conv2d(dim, dim , kernel_size=1, bias=bias)\n",
    "        self.q_dwconv = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, groups=dim, bias=bias)\n",
    "        self.kv = nn.Conv2d(dim, dim*2, kernel_size=1, bias=bias)\n",
    "        self.kv_dwconv = nn.Conv2d(dim*2, dim*2, kernel_size=3, stride=1, padding=1, groups=dim*2, bias=bias)\n",
    "        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        q = self.q_dwconv(self.q(x))\n",
    "        kv = self.kv_dwconv(self.kv(y))\n",
    "        k, v = kv.chunk(2, dim=1)\n",
    "\n",
    "        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "\n",
    "        q = torch.nn.functional.normalize(q, dim=-1)\n",
    "        k = torch.nn.functional.normalize(k, dim=-1)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.temperature\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        out = (attn @ v)\n",
    "\n",
    "        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n",
    "\n",
    "        out = self.project_out(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        # return out+x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_denoiser(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels = 3,\n",
    "                 out_channels = 3,\n",
    "                 embed_dim = 48,\n",
    "                 LayerNorm_type = \"WithBias\",\n",
    "                 ffn_expansion_factor = 2.66,\n",
    "                 bias = False):\n",
    "\n",
    "        super(my_denoiser, self).__init__()\n",
    "\n",
    "        self.patch_embed_1 = base.OverlapPatchEmbed(in_channels, embed_dim)\n",
    "        self.patch_embed_dwt_1 = base.OverlapPatchEmbed(in_channels, embed_dim) \n",
    "        self.T1 = base.TransformerBlock(dim = embed_dim, num_heads = 4, ffn_expansion_factor = ffn_expansion_factor, bias = bias, LayerNorm_type = LayerNorm_type)  \n",
    "        self.T1_dwt = base.TransformerBlock(dim = embed_dim, num_heads = 4, ffn_expansion_factor = ffn_expansion_factor, bias = bias, LayerNorm_type = LayerNorm_type)\n",
    "\n",
    "        self.cross_attn_1 = CrossAttention(dim = embed_dim, num_heads = 4, bias = bias)\n",
    "\n",
    "\n",
    "        \n",
    "        self.patch_embed_2 = base.OverlapPatchEmbed(in_channels, embed_dim)\n",
    "        self.patch_embed_dwt_2 = base.OverlapPatchEmbed(in_channels, embed_dim)\n",
    "        self.T2 = base.TransformerBlock(dim = embed_dim, num_heads = 4, ffn_expansion_factor = ffn_expansion_factor, bias = bias, LayerNorm_type = LayerNorm_type)\n",
    "        self.T2_dwt = base.TransformerBlock(dim = embed_dim, num_heads = 4, ffn_expansion_factor = ffn_expansion_factor, bias = bias, LayerNorm_type = LayerNorm_type)\n",
    "\n",
    "        self.cross_attn_2 = CrossAttention(dim = embed_dim, num_heads = 4, bias = bias)\n",
    "\n",
    "\n",
    "        self.patch_embed_3 = base.OverlapPatchEmbed(in_channels, embed_dim)\n",
    "        self.patch_embed_dwt_3 = base.OverlapPatchEmbed(in_channels, embed_dim)\n",
    "\n",
    "        self.T3 = base.TransformerBlock(dim = embed_dim , num_heads = 4, ffn_expansion_factor = ffn_expansion_factor, bias = bias, LayerNorm_type = LayerNorm_type)\n",
    "        self.T3_dwt = base.TransformerBlock(dim = embed_dim , num_heads = 4, ffn_expansion_factor = ffn_expansion_factor, bias = bias, LayerNorm_type = LayerNorm_type)\n",
    "\n",
    "        self.cross_attn_3 = CrossAttention(dim = embed_dim, num_heads = 4, bias = bias)\n",
    "\n",
    "        self.T4 = base.TransformerBlock(dim = embed_dim, num_heads = 4, ffn_expansion_factor = ffn_expansion_factor, bias = bias, LayerNorm_type = LayerNorm_type)\n",
    "\n",
    "        self.cross_attn_2_3 = CrossAttention(dim = embed_dim, num_heads = 4, bias = bias)  \n",
    "\n",
    "        self.cross_attn_1_23 = CrossAttention(dim = embed_dim, num_heads = 4, bias = bias)      \n",
    "\n",
    "        self.T5 = base.TransformerBlock(dim = embed_dim, num_heads = 4, ffn_expansion_factor = ffn_expansion_factor, bias = bias, LayerNorm_type = LayerNorm_type)\n",
    "\n",
    "        self.T6 = base.TransformerBlock(dim = embed_dim , num_heads = 4, ffn_expansion_factor = ffn_expansion_factor, bias = bias, LayerNorm_type = LayerNorm_type)\n",
    "\n",
    "\n",
    "        self.cross_attn_4_5 = CrossAttention(dim = embed_dim , num_heads = 4, bias = bias)    \n",
    "\n",
    "\n",
    "        self.cross_attn_6_45 = CrossAttention(dim = embed_dim , num_heads = 4, bias = bias)     \n",
    "\n",
    "        \n",
    "        self.out_project = nn.Conv2d(embed_dim, out_channels = out_channels, kernel_size = 1, bias = bias)\n",
    "\n",
    "\n",
    "    def forward(self, in_img):\n",
    "\n",
    "        #----scale 1-----#\n",
    "        in_enc_1 = self.patch_embed_1(in_img)\n",
    "        out_enc_1 = self.T1(in_enc_1)\n",
    "        LL_1, LH_1, HL_1, HH_1 = dwt_batch(in_img.detach())\n",
    "        dwt_1 = bicubic_upsample(LH_1.detach() + HL_1.detach() + HH_1.detach())\n",
    "        dwt_1 = dwt_1.to(DEVICE)\n",
    "        in_enc_1_dwt = self.patch_embed_dwt_1(dwt_1)\n",
    "        out_enc_1_dwt = self.T1_dwt(in_enc_1_dwt)\n",
    "\n",
    "\n",
    "        CA_1 = self.cross_attn_1(out_enc_1, out_enc_1_dwt)         \n",
    "\n",
    "    \n",
    "        LL_1 = (LL_1.detach()).to(DEVICE)\n",
    "        in_enc_2 = self.patch_embed_2(LL_1)\n",
    "        out_enc_2 = self.T2(in_enc_2)\n",
    "        LL_2, LH_2, HL_2, HH_2 = dwt_batch(LL_1.detach())\n",
    "        dwt_2 = bicubic_upsample(LH_2.detach() + HL_2.detach() + HH_2.detach())\n",
    "        dwt_2 = dwt_2.to(DEVICE)\n",
    "        in_enc_2_dwt = self.patch_embed_dwt_2(dwt_2)\n",
    "        out_enc_2_dwt = self.T2_dwt(in_enc_2_dwt)\n",
    "\n",
    "        CA_2 = self.cross_attn_2(out_enc_2, out_enc_2_dwt)    \n",
    "\n",
    "\n",
    "       \n",
    "        LL_2 = (LL_2.detach()).to(DEVICE)\n",
    "        in_enc_3 = self.patch_embed_3(LL_2)\n",
    "        out_enc_3 = self.T3(in_enc_3)\n",
    "        _ , LH_3, HL_3, HH_3 = dwt_batch(LL_2.detach())\n",
    "        dwt_3 = bicubic_upsample(LH_3.detach() + HL_3.detach() + HH_3.detach())\n",
    "        dwt_3 = dwt_3.to(DEVICE)\n",
    "\n",
    "        in_enc_3_dwt = self.patch_embed_dwt_3(dwt_3)\n",
    "        out_enc_3_dwt = self.T3_dwt(in_enc_3_dwt)\n",
    "\n",
    "        CA_3 = self.cross_attn_3(out_enc_3, out_enc_3_dwt) \n",
    "\n",
    "        t4 = self.T4(CA_3)              \n",
    "        t4 = t4 + CA_3  \n",
    "\n",
    "\n",
    "        CA_3_up = bicubic_upsample(CA_3)\n",
    "\n",
    "\n",
    "        CA_2_3 = self.cross_attn_2_3(CA_2, CA_3_up) \n",
    "\n",
    "        CA_2_3 = CA_2_3 + CA_2\n",
    "\n",
    "        t5 = self.T5(CA_2_3)            \n",
    "\n",
    "        t5 = t5 + CA_2_3 \n",
    "\n",
    "        CA_2_3_up = bicubic_upsample(CA_2_3)\n",
    "\n",
    "        CA_1_23 = self.cross_attn_1_23(CA_1,CA_2_3_up)\n",
    "\n",
    "\n",
    "        CA_1_23 = CA_1_23 + CA_1\n",
    "\n",
    "        t6 = self.T5(CA_1_23)   \n",
    "\n",
    "        t6 = t6+ CA_1_23 \n",
    "\n",
    "        t4_up = bicubic_upsample(t4)\n",
    "\n",
    "        CA_4_5 = self.cross_attn_4_5(t5,t4_up)  \n",
    "\n",
    "\n",
    "        CA_4_5 = CA_4_5 + t5\n",
    "\n",
    "        CA_4_5_up = bicubic_upsample(CA_4_5)\n",
    "\n",
    "        CA_6_45 = self.cross_attn_6_45(t6, CA_4_5_up)  \n",
    "\n",
    "\n",
    "        CA_6_45 = CA_6_45 + t6\n",
    "\n",
    "        out = self.out_project(CA_6_45)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharbonnierLoss(nn.Module):\n",
    "    \"\"\"Charbonnier Loss (L1)\"\"\"\n",
    "\n",
    "    def __init__(self, eps=1e-3):\n",
    "        super(CharbonnierLoss, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        diff = x - y\n",
    "        # loss = torch.sum(torch.sqrt(diff * diff + self.eps))\n",
    "        loss = torch.mean(torch.sqrt((diff * diff) + (self.eps*self.eps)))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of Denoiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoiser = my_denoiser().to(DEVICE)\n",
    "criterion = CharbonnierLoss().cuda()\n",
    "optimizer = torch.optim.Adam(denoiser.parameters(), lr = 0.004, betas = (0.9, 0.999), eps = 1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training block\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "best_loss = 100\n",
    "\n",
    "for epoch in range(15):\n",
    "    epoch_start_time = time.time()\n",
    "    epoch_loss = 0\n",
    "    train_id = 1\n",
    "\n",
    "\n",
    "    for i, data in enumerate(tqdm(adv_train_loader), 0):\n",
    "        # zero_grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        target = data[0].cuda()     # first element is clean\n",
    "        input_ = data[1].cuda()     # 2nd element is noisy\n",
    "\n",
    "        restored = denoiser(input_)\n",
    "        restored = restored.to(DEVICE)\n",
    "        loss = criterion(restored, target)\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        epoch_loss +=loss.item()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        denoiser.eval()\n",
    "        correct_pred, num_examples = 0, 0\n",
    "\n",
    "        for ii, data_val in enumerate((adv_test_loader), 0):\n",
    "            clean = data_val[0].cuda()\n",
    "            noisy = data_val[1].cuda()\n",
    "            targets = data_val[2].cuda()\n",
    "\n",
    "            restored = denoiser(noisy).detach()\n",
    "            # restored = torch.clamp(restored,0,1)\n",
    "            restored = restored.to(DEVICE)\n",
    "\n",
    "            outputs = classifier(restored)\n",
    "            _, predicted_labels = outputs.max(1)\n",
    "            num_examples += targets.size(0)\n",
    "            correct_pred += (predicted_labels == targets).sum()\n",
    "\n",
    "        acc = correct_pred.float()/num_examples * 100\n",
    "        print(\"The accuracy at current level is \", acc)\n",
    "\n",
    "        denoiser.train()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "    print(\"Epoch: {}\\tTime: {:.4f}\\tLoss: {:.4f}\".format(epoch, time.time()-epoch_start_time,epoch_loss))\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "\n",
    "    if epoch_loss<best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(denoiser, best_denoiser_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversrial Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=200, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
    "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
    "parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\n",
    "parser.add_argument(\"--img_size\", type=int, default=32, help=\"size of each image dimension\")\n",
    "parser.add_argument(\"--channels\", type=int, default=1, help=\"number of image channels\")\n",
    "parser.add_argument(\"--n_critic\", type=int, default=5, help=\"number of training steps for discriminator per iter\")\n",
    "parser.add_argument(\"--clip_value\", type=float, default=0.01, help=\"lower and upper clip value for disc. weights\")\n",
    "parser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval betwen image samples\")\n",
    "parser.add_argument(\"--n_residual_blocks\", type=int, default=9, help=\"number of residual blocks in generator\")\n",
    "\n",
    "parser.add_argument('-f')\n",
    "opt_1 = parser.parse_args()\n",
    "print(opt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adversarial training\n",
    "\n",
    "auxiliary_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "classifier_1 = torch.load(\"trained_classifier_path\", map_location='cuda')\n",
    "optimizer_C = torch.optim.Adam(classifier_1.parameters(), lr=opt_1.lr, betas=(opt_1.b1, opt_1.b2))\n",
    "\n",
    "classifier_1.cuda()\n",
    "\n",
    "auxiliary_loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_denoiser = torch.load(best_denoiser_path, map_location = DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "batches_done = 0\n",
    "opt_1.n_epochs = 20\n",
    "\n",
    "for epoch in range(opt_1.n_epochs):\n",
    "\n",
    "    torch.save(classifier_1, classifier_stage_1_path)\n",
    "\n",
    "    for i, data in enumerate(adv_train_loader):\n",
    "\n",
    "\n",
    "        clean = data[0].to(DEVICE)\n",
    "        noisy = data[1].to(DEVICE)\n",
    "        labels = data[2].to(DEVICE)\n",
    "\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            gen_imgs = best_denoiser(noisy).detach()\n",
    "\n",
    "        gen_imgs = gen_imgs.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "        optimizer_C.zero_grad()\n",
    "\n",
    "        # Loss for real images\n",
    "        real_aux = classifier_1(clean)\n",
    "        c_real_loss = auxiliary_loss(real_aux, labels)\n",
    "\n",
    "        # Loss for fake images\n",
    "        fake_aux = classifier_1(gen_imgs.float())\n",
    "        fake_aux = F.log_softmax(fake_aux,dim=1)      \n",
    "        # log_softmax is done for numerical stability\n",
    "        c_fake_loss = auxiliary_loss(fake_aux, labels)\n",
    "\n",
    "        \n",
    "        c_loss = c_fake_loss\n",
    "       \n",
    "        # Calculate classifier accuracy\n",
    "        pred = np.concatenate([real_aux.data.cpu().numpy()], axis=0)\n",
    "        gt = np.concatenate([labels.data.cpu().numpy()], axis=0)\n",
    "        c_acc_real = np.mean(np.argmax(pred, axis=1) == gt)     # classifier accuracy for real images\n",
    "\n",
    "        pred = np.concatenate([fake_aux.data.cpu().numpy()], axis=0)\n",
    "        gt = np.concatenate([labels.data.cpu().numpy()], axis=0)\n",
    "        c_acc_generated = np.mean(np.argmax(pred, axis=1) == gt)  # classifer accuracy for fake images\n",
    "\n",
    "        c_loss.backward(retain_graph=True)      \n",
    "\n",
    "        print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [C loss: %f, real_acc: %d%% gen_acc %d%%]\"\n",
    "            % (epoch, opt_1.n_epochs, i, len(adv_train_loader), c_loss.item(), 100 * c_acc_real,100*c_acc_generated)\n",
    "        )\n",
    "        batches_done = epoch * len(adv_train_loader) + i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgsm = torchattacks.FGSM(classifier, eps=0.15)\n",
    "pgd = torchattacks.PGD(classifier, eps=0.1, alpha=0.1, steps=15)\n",
    "upgd = torchattacks.UPGD(classifier, eps=0.1,alpha=0.1,steps=15)\n",
    "mifgsm = torchattacks.MIFGSM(classifier,eps=0.2,steps = 15,decay = 1)\n",
    "bim = torchattacks.BIM(classifier, eps=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAdversarialDataset():\n",
    "    def __init__(self, adversarial_data, transform=None):\n",
    "        self.adversarial_data = adversarial_data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.adversarial_data[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attacked_dataset(loader,atk):\n",
    "\n",
    "    atk_data = []\n",
    "\n",
    "    for i, (imgs, labels) in enumerate(loader):\n",
    "\n",
    "            # Configure input\n",
    "            imgs = imgs.to(DEVICE)\n",
    "\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "\n",
    "            attacked_imgs = atk(imgs,labels).detach()\n",
    "\n",
    "            atk_data.extend(list(zip(imgs,attacked_imgs,labels)))\n",
    "\n",
    "    test_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    # Create a custom dataset\n",
    "    adversarial_dataset = CustomAdversarialDataset(atk_data, transform=test_transform)\n",
    "\n",
    "    # Create a DataLoader for testing\n",
    "    batch_size = 256  # Adjust this as needed\n",
    "    test_loader = DataLoader(atk_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgsm_atk = attacked_dataset(test_loader, fgsm)\n",
    "pgd_atk = attacked_dataset(test_loader, pgd)\n",
    "upgd_atk = attacked_dataset(test_loader, upgd)\n",
    "mifgsm_atk = attacked_dataset(test_loader, mifgsm)\n",
    "bim_atk = attacked_dataset(test_loader, bim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_2 = torch.load(classifier_stage_1_path, map_location = DEVICE)    # the adversirally trained classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_clean(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "\n",
    "    for i, data in enumerate(data_loader):\n",
    "\n",
    "        clean = data[0].to(device)       # clean images\n",
    "        targets = data[2].to(device)\n",
    "\n",
    "        logits, probas = model.my_forward(clean)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_attacked(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "\n",
    "    for i, data in enumerate(data_loader):\n",
    "\n",
    "        noisy = data[1].to(device)       # noisy images\n",
    "        targets = data[2].to(device)\n",
    "\n",
    "        logits, probas = model.my_forward(noisy)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_denoised(model, denoiser, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "\n",
    "    for i, data in enumerate(data_loader):\n",
    "\n",
    "        clean = data[0].to(device)\n",
    "        noisy = data[1].to(device)\n",
    "        targets = data[2].to(device)\n",
    "\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            restored = denoiser(noisy).detach()\n",
    "\n",
    "        restored = torch.clamp(restored,0,1)\n",
    "        restored = restored.to(DEVICE)\n",
    "\n",
    "\n",
    "        logits, probas = model.my_forward(restored.float())\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FGSM\")\n",
    "\n",
    "clean_acc = compute_accuracy_clean(classifier, fgsm_atk, DEVICE)\n",
    "atk_acc = compute_accuracy_attacked(classifier, fgsm_atk, DEVICE)\n",
    "denoised_acc = compute_accuracy_denoised(classifier_2, best_denoiser,fgsm_atk, DEVICE)   # using classifier after adversarial training\n",
    "\n",
    "\n",
    "print(\"Clean Accuracy\", clean_acc.item(),\"%\")\n",
    "print(\"Attacked Accuracy\", atk_acc.item(),\"%\")\n",
    "print(\"Denoised Accuracy\", denoised_acc.item(),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PGD\")\n",
    "\n",
    "clean_acc = compute_accuracy_clean(classifier, pgd_atk, DEVICE)\n",
    "atk_acc = compute_accuracy_attacked(classifier, pgd_atk, DEVICE)\n",
    "denoised_acc = compute_accuracy_denoised(classifier, best_denoiser, pgd_atk, DEVICE)   # using classifier after adversarial training\n",
    "\n",
    "\n",
    "\n",
    "print(\"Clean Accuracy\", clean_acc.item(),\"%\")\n",
    "print(\"Attacked Accuracy\", atk_acc.item(),\"%\")\n",
    "print(\"Denoised Accuracy\", denoised_acc.item(),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"UPGD\")\n",
    "\n",
    "clean_acc = compute_accuracy_clean(classifier, upgd_atk, DEVICE)\n",
    "atk_acc = compute_accuracy_attacked(classifier, upgd_atk, DEVICE)\n",
    "denoised_acc = compute_accuracy_denoised(classifier_2, best_denoiser, upgd_atk, DEVICE)   # using classifier after adversarial training\n",
    "\n",
    "print(\"Clean Accuracy\", clean_acc.item(),\"%\")\n",
    "print(\"Attacked Accuracy\", atk_acc.item(),\"%\")\n",
    "print(\"Denoised Accuracy\", denoised_acc.item(),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MIFGSM\")\n",
    "\n",
    "clean_acc = compute_accuracy_clean(classifier, mifgsm_atk, DEVICE)\n",
    "atk_acc = compute_accuracy_attacked(classifier, mifgsm_atk, DEVICE)\n",
    "denoised_acc = compute_accuracy_denoised(classifier_2, best_denoiser,mifgsm_atk, DEVICE)   # using classifier after adversarial training\n",
    "\n",
    "print(\"Clean Accuracy\", clean_acc.item(),\"%\")\n",
    "print(\"Attacked Accuracy\", atk_acc.item(),\"%\")\n",
    "print(\"Denoised Accuracy\", denoised_acc.item(),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BIM\")\n",
    "\n",
    "clean_acc = compute_accuracy_clean(classifier, bim_atk, DEVICE)\n",
    "atk_acc = compute_accuracy_attacked(classifier, bim_atk, DEVICE)\n",
    "denoised_acc = compute_accuracy_denoised(classifier_2, best_denoiser,bim_atk, DEVICE)   # using classifier after adversarial training\n",
    "\n",
    "print(\"Clean Accuracy\", clean_acc.item(),\"%\")\n",
    "print(\"Attacked Accuracy\", atk_acc.item(),\"%\")\n",
    "print(\"Denoised Accuracy\", denoised_acc.item(),\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
